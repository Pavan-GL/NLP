{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76999f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0888024",
   "metadata": {},
   "source": [
    "SpaCy is much more than an NLP framework. It is also a way of designing and implementing complex pipelines. A pipeline is a sequence of pipes, or actors on data, that make alterations to the data or extract information from it. In some cases, later pipes require the output from earlier pipes. In other cases, a pipe can exist entirely on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f7d96",
   "metadata": {},
   "source": [
    "### Attribute Rulers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144df52",
   "metadata": {},
   "source": [
    "Dependency Parser\n",
    "\n",
    "EntityLinker\n",
    "\n",
    "EntityRecognizer\n",
    "\n",
    "EntityRuler\n",
    "\n",
    "Lemmatizer\n",
    "\n",
    "Morpholog\n",
    "\n",
    "SentenceRecognizer\n",
    "\n",
    "Sentencizer\n",
    "\n",
    "SpanCategorizer\n",
    "\n",
    "Tagger\n",
    "\n",
    "TextCategorizer\n",
    "\n",
    "Tok2Vec\n",
    "\n",
    "Tokenizer\n",
    "\n",
    "TrainablePipe\n",
    "\n",
    "Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3e13f",
   "metadata": {},
   "source": [
    "### Matchers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61643107",
   "metadata": {},
   "source": [
    "DependencyMatcher\n",
    "\n",
    "Matcher\n",
    "\n",
    "PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb7970",
   "metadata": {},
   "source": [
    "### How to Add Pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc50ab78",
   "metadata": {},
   "source": [
    "In most cases, you will use an off-the-shelf spaCy model. In some cases, however, an off-the-shelf model will not fill your needs or will perform a specific task very slowly. A good example of this is sentence tokenization. Imagine if you had a document that was around 1 million sentences long. Even if you used the small English model, your model would take a long time to process those 1 million sentences and separate them. In this instance, you would want to make a blank English model and simply add the Sentencizer to it. The reason is because each pipe in a pipeline will be activated (unless specified) and that means that each pipe from Dependency Parser to named entity recognition will be performed on your data. This is a serious waste of computational resources and time. The small model may take hours to achieve this task. By creating a blank model and simply adding a Sentencizer to it, you can reduce this time to merely minutes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3961c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To demonstrate this process, letâ€™s first create a blank model.\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c603c34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x21c76205d80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697fd96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "s = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
    "soup = BeautifulSoup(s.content).text.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "nlp.max_length = 5278439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09595911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94133\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = nlp(soup)\n",
    "print (len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3008881",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "nlp2.max_length = 5278439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "doc = nlp2(soup)\n",
    "print (len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c8304",
   "metadata": {},
   "source": [
    "### Examining a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ac98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de954c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
