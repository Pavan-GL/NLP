{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247afa42",
   "metadata": {},
   "source": [
    "The Python library spaCy offers a few different methods for performing rules-based NER. One such method is via its EntityRuler.\n",
    "\n",
    "The EntityRuler is a spaCy factory that allows one to create a set of patterns with corresponding labels. A factory in spaCy is a set of classes and functions preloaded in spaCy that perform set tasks. In the case of the EntityRuler, the factory at hand allows the user to create an EntityRuler, give it a set of instructions, and then use this instructions to find and label entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5181a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treblinka LOC\n",
      "Poland GPE\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Sample text\n",
    "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
    "\n",
    "#Create the Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee6b43c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treblinka LOC\n",
      "Poland GPE\n",
      "Treblinka GPE\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Sample text\n",
    "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
    "\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7f455a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'entity_ruler': []},\n",
       " 'attrs': {'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c44cb6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treblinka LOC\n",
      "Poland GPE\n",
      "Treblinka GPE\n"
     ]
    }
   ],
   "source": [
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Sample text\n",
    "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
    "\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", after=\"ner\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78a3f0",
   "metadata": {},
   "source": [
    "### Complex Rules and Variance to the EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2602f01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(555) 555-5555 PHONE_NUMBER\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Sample text\n",
    "text = \"This is a sample number (555) 555-5555.\"\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Create the Ruler and Add it\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "                {\"label\": \"PHONE_NUMBER\", \"pattern\": [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddd\"},\n",
    "                {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]}\n",
    "            ]\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "\n",
    "#create the doc\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3156d122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
